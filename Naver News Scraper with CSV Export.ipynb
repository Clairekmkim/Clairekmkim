{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA83tf1Skt9urW5wtJizwK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Clairekmkim/Clairekmkim/blob/main/Naver%20News%20Scraper%20with%20CSV%20Export.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "csN9jOptDOYe",
        "outputId": "a1b09f22-99e3-45c0-ba2e-a587f17c37bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "결과를 저장할 디렉토리 경로를 입력하세요 (엔터를 누르면 현재 디렉토리에 저장됩니다): \n",
            "페이지 1 완료 (항목 수: 10)\n",
            "페이지 2 완료 (항목 수: 10)\n",
            "페이지 3 완료 (항목 수: 10)\n",
            "페이지 4 완료 (항목 수: 10)\n",
            "페이지 5 완료 (항목 수: 10)\n",
            "페이지 6 완료 (항목 수: 10)\n",
            "페이지 7 완료 (항목 수: 10)\n",
            "페이지 8 완료 (항목 수: 10)\n",
            "페이지 9 완료 (항목 수: 10)\n",
            "페이지 10 완료 (항목 수: 10)\n",
            "페이지 11 완료 (항목 수: 10)\n",
            "페이지 12 완료 (항목 수: 10)\n",
            "페이지 13 완료 (항목 수: 10)\n",
            "페이지 14 완료 (항목 수: 10)\n",
            "페이지 15 완료 (항목 수: 10)\n",
            "페이지 16 완료 (항목 수: 10)\n",
            "페이지 17 완료 (항목 수: 10)\n",
            "페이지 18 완료 (항목 수: 10)\n",
            "페이지 19 완료 (항목 수: 10)\n",
            "페이지 20 완료 (항목 수: 10)\n",
            "200개의 뉴스 항목이 '/content/naver_news_리벨리온_박성현.csv' 파일에 저장되었습니다.\n",
            "현재 작업 디렉토리: /content\n",
            "현재 디렉토리의 파일 목록:\n",
            "['.config', 'naver_news_리벨리온_박성현.csv', 'sample_data']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f6ac0ca1-4199-49e8-a962-046aeb326064\", \"naver_news_\\ub9ac\\ubca8\\ub9ac\\uc628_\\ubc15\\uc131\\ud604.csv\", 109463)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import quote_plus\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "\n",
        "def scrape_naver_news(keyword1, keyword2, num_pages=20):\n",
        "    news_data = []\n",
        "    base_url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={}&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start={}\"\n",
        "\n",
        "    query = quote_plus(f\"{keyword1} {keyword2}\")\n",
        "\n",
        "    user_agents = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0'\n",
        "    ]\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        start = (page - 1) * 10 + 1\n",
        "        url = base_url.format(query, start)\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': random.choice(user_agents),\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Referer': 'https://www.naver.com/',\n",
        "            'DNT': '1',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        }\n",
        "\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = session.get(url, headers=headers, timeout=15)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                news_items = soup.find_all('div', class_='news_area')\n",
        "\n",
        "                if not news_items:\n",
        "                    print(f\"페이지 {page}에서 뉴스 항목을 찾을 수 없습니다.\")\n",
        "                    break\n",
        "\n",
        "                for item in news_items:\n",
        "                    title_elem = item.find('a', class_='news_tit')\n",
        "                    content_elem = item.find('div', class_='news_dsc')\n",
        "\n",
        "                    if title_elem and content_elem:\n",
        "                        title = title_elem.text.strip()\n",
        "                        content = content_elem.text.strip()\n",
        "                        link = title_elem['href']\n",
        "\n",
        "                        news_data.append({\n",
        "                            'title': title,\n",
        "                            'content': content,\n",
        "                            'link': link\n",
        "                        })\n",
        "\n",
        "                print(f\"페이지 {page} 완료 (항목 수: {len(news_items)})\")\n",
        "                break  # 성공적으로 데이터를 가져왔으므로 재시도 루프를 종료\n",
        "            except requests.RequestException as e:\n",
        "                print(f\"페이지 {page} 스크래핑 중 오류 발생 (시도 {attempt + 1}/{max_retries}): {e}\")\n",
        "                if attempt == max_retries - 1:\n",
        "                    print(f\"페이지 {page} 스크래핑 실패\")\n",
        "                else:\n",
        "                    time.sleep(random.uniform(5, 10))  # 재시도 전 더 긴 대기 시간\n",
        "\n",
        "        # 페이지 간 랜덤한 지연 시간 (5-10초)\n",
        "        time.sleep(random.uniform(5, 10))\n",
        "\n",
        "    return news_data\n",
        "\n",
        "def save_to_csv(data, filepath):\n",
        "    if not data:\n",
        "        print(\"저장할 데이터가 없습니다.\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "    keys = data[0].keys()\n",
        "    with open(filepath, 'w', newline='', encoding='utf-8-sig') as output_file:\n",
        "        dict_writer = csv.DictWriter(output_file, keys)\n",
        "        dict_writer.writeheader()\n",
        "        dict_writer.writerows(data)\n",
        "\n",
        "    print(f\"{len(data)}개의 뉴스 항목이 '{filepath}' 파일에 저장되었습니다.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    keyword1 = \"리벨리온\"\n",
        "    keyword2 = \"박성현\"\n",
        "    num_pages = 20  # 스크래핑할 페이지 수를 20으로 증가\n",
        "\n",
        "    save_dir = input(\"결과를 저장할 디렉토리 경로를 입력하세요 (엔터를 누르면 현재 디렉토리에 저장됩니다): \").strip()\n",
        "    if not save_dir:\n",
        "        save_dir = os.getcwd()\n",
        "\n",
        "    filename = f\"naver_news_{keyword1}_{keyword2}.csv\"\n",
        "    filepath = os.path.join(save_dir, filename)\n",
        "\n",
        "    news_data = scrape_naver_news(keyword1, keyword2, num_pages)\n",
        "\n",
        "    if news_data:\n",
        "        save_to_csv(news_data, filepath)\n",
        "    else:\n",
        "        print(\"스크래핑된 뉴스 데이터가 없습니다.\")\n",
        "\n",
        "    print(f\"현재 작업 디렉토리: {os.getcwd()}\")\n",
        "    print(\"현재 디렉토리의 파일 목록:\")\n",
        "    print(os.listdir())\n",
        "\n",
        "# Google Colab 사용 시\n",
        "from google.colab import files\n",
        "files.download(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import quote_plus\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "\n",
        "def scrape_naver_news(keyword1, keyword2, num_pages=20):\n",
        "    news_data = []\n",
        "    base_url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={}&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start={}\"\n",
        "\n",
        "    query = quote_plus(f\"{keyword1} {keyword2}\")\n",
        "\n",
        "    user_agents = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0'\n",
        "    ]\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        start = (page - 1) * 10 + 1\n",
        "        url = base_url.format(query, start)\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': random.choice(user_agents),\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Referer': 'https://www.naver.com/',\n",
        "            'DNT': '1',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        }\n",
        "\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = session.get(url, headers=headers, timeout=15)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                news_items = soup.find_all('div', class_='news_area')\n",
        "\n",
        "                if not news_items:\n",
        "                    print(f\"페이지 {page}에서 뉴스 항목을 찾을 수 없습니다.\")\n",
        "                    break\n",
        "\n",
        "                for item in news_items:\n",
        "                    title_elem = item.find('a', class_='news_tit')\n",
        "                    content_elem = item.find('div', class_='news_dsc')\n",
        "                    date_elem = item.find('span', class_='info')  # 날짜 정보를 포함하는 요소\n",
        "\n",
        "                    if title_elem and content_elem:\n",
        "                        title = title_elem.text.strip()\n",
        "                        content = content_elem.text.strip()\n",
        "                        link = title_elem['href']\n",
        "                        date = date_elem.text.strip() if date_elem else \"날짜 정보 없음\"\n",
        "\n",
        "                        news_data.append({\n",
        "                            'title': title,\n",
        "                            'content': content,\n",
        "                            'link': link,\n",
        "                            'date': date  # 날짜 정보 추가\n",
        "                        })\n",
        "\n",
        "                print(f\"페이지 {page} 완료 (항목 수: {len(news_items)})\")\n",
        "                break  # 성공적으로 데이터를 가져왔으므로 재시도 루프를 종료\n",
        "            except requests.RequestException as e:\n",
        "                print(f\"페이지 {page} 스크래핑 중 오류 발생 (시도 {attempt + 1}/{max_retries}): {e}\")\n",
        "                if attempt == max_retries - 1:\n",
        "                    print(f\"페이지 {page} 스크래핑 실패\")\n",
        "                else:\n",
        "                    time.sleep(random.uniform(5, 10))  # 재시도 전 더 긴 대기 시간\n",
        "\n",
        "        # 페이지 간 랜덤한 지연 시간 (5-10초)\n",
        "        time.sleep(random.uniform(5, 10))\n",
        "\n",
        "    return news_data\n",
        "\n",
        "def save_to_csv(data, filepath):\n",
        "    if not data:\n",
        "        print(\"저장할 데이터가 없습니다.\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "    keys = data[0].keys()\n",
        "    with open(filepath, 'w', newline='', encoding='utf-8-sig') as output_file:\n",
        "        dict_writer = csv.DictWriter(output_file, keys)\n",
        "        dict_writer.writeheader()\n",
        "        dict_writer.writerows(data)\n",
        "\n",
        "    print(f\"{len(data)}개의 뉴스 항목이 '{filepath}' 파일에 저장되었습니다.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    keyword1 = \"리벨리온\"\n",
        "    keyword2 = \"박성현\"\n",
        "    num_pages = 20\n",
        "\n",
        "    save_dir = input(\"결과를 저장할 디렉토리 경로를 입력하세요 (엔터를 누르면 현재 디렉토리에 저장됩니다): \").strip()\n",
        "    if not save_dir:\n",
        "        save_dir = os.getcwd()\n",
        "\n",
        "    filename = f\"naver_news_{keyword1}_{keyword2}.csv\"\n",
        "    filepath = os.path.join(save_dir, filename)\n",
        "\n",
        "    news_data = scrape_naver_news(keyword1, keyword2, num_pages)\n",
        "\n",
        "    if news_data:\n",
        "        save_to_csv(news_data, filepath)\n",
        "    else:\n",
        "        print(\"스크래핑된 뉴스 데이터가 없습니다.\")\n",
        "\n",
        "    print(f\"현재 작업 디렉토리: {os.getcwd()}\")\n",
        "    print(\"현재 디렉토리의 파일 목록:\")\n",
        "    print(os.listdir())\n",
        "\n",
        "# Google Colab 사용 시\n",
        "from google.colab import files\n",
        "files.download(filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "VWwXwPdeSi6s",
        "outputId": "24625466-82a1-46ae-9d91-0d13255e8ef9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "결과를 저장할 디렉토리 경로를 입력하세요 (엔터를 누르면 현재 디렉토리에 저장됩니다): \n",
            "페이지 1 완료 (항목 수: 10)\n",
            "페이지 2 완료 (항목 수: 10)\n",
            "페이지 3 완료 (항목 수: 10)\n",
            "페이지 4 완료 (항목 수: 10)\n",
            "페이지 5 완료 (항목 수: 10)\n",
            "페이지 6 완료 (항목 수: 10)\n",
            "페이지 7 완료 (항목 수: 10)\n",
            "페이지 8 완료 (항목 수: 10)\n",
            "페이지 9 완료 (항목 수: 10)\n",
            "페이지 10 완료 (항목 수: 10)\n",
            "페이지 11 완료 (항목 수: 10)\n",
            "페이지 12 완료 (항목 수: 10)\n",
            "페이지 13 완료 (항목 수: 10)\n",
            "페이지 14 완료 (항목 수: 10)\n",
            "페이지 15 완료 (항목 수: 10)\n",
            "페이지 16 완료 (항목 수: 10)\n",
            "페이지 17 완료 (항목 수: 10)\n",
            "페이지 18 완료 (항목 수: 10)\n",
            "페이지 19 완료 (항목 수: 10)\n",
            "페이지 20 완료 (항목 수: 10)\n",
            "200개의 뉴스 항목이 '/content/naver_news_리벨리온_박성현.csv' 파일에 저장되었습니다.\n",
            "현재 작업 디렉토리: /content\n",
            "현재 디렉토리의 파일 목록:\n",
            "['.config', 'naver_news_리벨리온_박성현.csv', 'sample_data']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b62553f5-0257-4fe9-b1f3-8c44753190d7\", \"naver_news_\\ub9ac\\ubca8\\ub9ac\\uc628_\\ubc15\\uc131\\ud604.csv\", 111691)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}